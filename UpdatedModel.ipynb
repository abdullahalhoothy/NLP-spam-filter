{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "fiver_readme_article.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk5x96V3t7Yx",
        "colab_type": "text"
      },
      "source": [
        "# Ham or Spam? \n",
        "## Introduction to NLP with Python\n",
        "\n",
        "In this article, we will be looking at one of the basics of Natural Language Processing, which is to train a classifier that is able to differentiate one class from another, in this case Spam or not-Spam. We will have to extract features from text, and select a classification algorithm that works best for us. We will be using Python 3 and the dataset is the SMS Spam Collection which tags 5,574 text messages based on whether they are “spam” or “ham” (not spam).\n",
        "\n",
        "Our goal is to build a predictive model which will determine whether a text message is spam or ham. \n",
        "\n",
        "![Mailbox](https://cdn.pixabay.com/photo/2015/11/17/23/33/mail-1048452__340.jpg)\n",
        "\n",
        "##What is Spam?\n",
        "\n",
        "Spam is, according to wikipedia, it's described as “the use of electronic messaging systems to send unsolicited bulk messages, especially advertising, indiscriminately.”  The word was coined sometime in 2001 or 2002, by the guys working on SpamBayes, the Python probabilistic classifier. Although there are more formal definitions, the key word is “unsolicited”. This means that you did not ask for messages from this source. So if you didn’t ask for the mail it must be spam, Right? Maybe, but if we are looking to differentiate one class from another, we need to start finding patterns.\n",
        "\n",
        "##The data and features\n",
        "\n",
        "Taking a first look at the data will give us valuable insights and if we do it in a systematic way it is called EDA, Exploratory Data Analysis. When building a classificator the most important value to look at is how balanced is our data. This is because it gives us a sense on how much our classificator is helping us. For example, if we have a 90% of class A and 10% of B, we may have a classificator with 90% accuracy that hasn’t learned anything besides predicting A every time it sees something new.\n",
        "\n",
        "This particular data set also has 4821 messages labelled as “ham” and 746 messages labelled as “spam”, which is the 87% and 13% repectlively. The class imbalance will become important later when assessing the strength of our classifier.\n",
        "\n",
        "Another thing we are looking at is the length of the message and the ratio of punctuation to letters. The length of the message will give us an idea of how many characters we will be dealing in every message, and the ratio of punctuation are features that we will be using to predict whether they are spam or not. \n",
        "\n",
        "We will be looking to extract every feature that allows us to classify better, and remove everything that makes us do worse. This is the idea that we will apply when removing stop words and stemming words.\n",
        "\n",
        "This is sometimes what we are looking for when we perform normalization of the data. When used correctly, it reduces noise, groups terms with similar semantic meanings and reduces computational costs by giving us a smaller matrix to work with. This matrix will be obtained using a vectorizer. Let’s dive into a more detailed explanation of each method.\n",
        "\n",
        "\n",
        "###Stopwords\n",
        "\n",
        "When going through text, there are words that are used all the time like connectors, articles and so on. In natural language processing, stop words are words which are filtered out before or after processing of data. These are some of the most common, short function words, such as the, is, at, which, and on. The basic idea is that if they are present in most of the texts, they are not adding information that allow us to see what makes every class different.\n",
        "\n",
        "Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. In our case we will be using the list of words that NLTK library provides. \n",
        "\n",
        "Any group of words can be chosen as the stop words for a given purpose, and we can also add custom words to our list. \n",
        "\n",
        "###Stemming \n",
        "\n",
        "Stemming is used to reduce every word into its root to group words that mean the same thing. The idea of stemming is a sort of normalizing method. We will use it to group words that have the same root but are expressed in a different tense. Many variations of words carry the same meaning, other than when tense is involved.\n",
        "\n",
        "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
        "\n",
        "Consider the phrase “I was taking a ride in the car” and “I was riding in the car”. This sentence means the same thing. The difference lies in the tense or termination of the verb “ride”. The “ing” at the end of “riding” denotes a clear past-tense, so is it truly necessary to differentiate between ride and riding, in the case of just trying to figure out the meaning of what this past-tense activity was? No, not really.\n",
        "\n",
        "This might be just one minor example, but imagine every word in the English, every possible tense and affix you can put on a word. Having individual dictionary entries per version would be highly redundant and inefficient, especially since, once we convert to numbers, the \"value\" is going to be identical.\n",
        "\n",
        "For our example, we will be using the Porter stemmer implemented in the NLTK library, one of the most popular stemming algorithms, which has been around since 1979.\n",
        "\n",
        "###TF-IDF\n",
        "\n",
        "Term Frequency - Inverse Document Frequency (TF-IDF) is a statistical measure that tries to evaluate how relevant a word is to a document in a collection of documents. It has many uses, most importantly in Natural Language Processing, where used to scoring words in machine learning algorithms.\n",
        "\n",
        "The relevance of each word is analyzed by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
        "\n",
        "![tf-idf](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRLrPdFEOeivuKLDtEb0ZpJUAfm6bHl7mo5th5gRi_MI3ZpAcKkNtUTpHQyv9uDFtkJ0d2Du8iiAIGMTY9DWO6oWI0Zg5dJODc8Qw&usqp=CAU&ec=45690272)\n",
        "\n",
        "Term frequency is the number of times a word appears in a document (in this case document meaning each message) divided by the total number of words in the document. Every document has its own term frequency. The inverse document frequency is the second term which evaluates how frequent is this term in the rest of the documents.\n",
        "\n",
        "\n",
        "TF-IDF was invented for document search and information retrieval and it works by assigning a value to each word and increasing it proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
        "\n",
        "However, if a certain word appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. \n",
        "\n",
        "###Vectorizing the Text with TF-IDF\n",
        "\n",
        "We need to transform the text into something that the algorithm can work with, and those are numeric vectors. The process of turning text into numbers is commonly known as vectorization or embedding. Vectorizers are functions which map words onto vectors of real numbers. The vectors form a vector space where all the rules of vector addition and measures of similarities apply. We will use a vectorizer which transforms a text into a vector representation given a certain method, in this case TF-IDF. \n",
        "\n",
        "While most vectorizers have their unique advantages, it is not always clear which one to use. In this case, the TF-IDF vectorizer was chosen for its simplicity and efficiency in vectorizing documents such as text messages.\n",
        "\n",
        "##Building a classifier and choosing and algorithm\n",
        "\n",
        "\n",
        "The next step is to select the type of classification algorithm to use. We will choose two candidate classifiers and evaluate them against the testing set to see which one works the best. For this we have selected two algorithms which are Random Forest and Gradient Boosting, both implemented in the Scikit-learn library.\n",
        "\n",
        "###Random Forest\n",
        "\n",
        "Let’s understand the algorithm in layman’s terms. Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest consists of trees. It is said that the more trees it has, the more robust a forest is. The Random forests algorithm creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n",
        "\n",
        "It technically is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result. In the case of regression, the average of all the tree outputs is considered as the final result. It is simpler and more powerful compared to the other non-linear classification algorithms.\n",
        "\n",
        "The algorithm works in four steps:\n",
        "\n",
        "- Select random samples from a given dataset.\n",
        "- Construct a decision tree for each sample and get a prediction result from - each decision tree.\n",
        "- Perform a vote for each predicted result.\n",
        "- Select the prediction result with the most votes as the final prediction.\n",
        "\n",
        "It has advantages like:\n",
        "- Is considered to be highly accurate and robust because of the number of - decision trees participating in the process.\n",
        "- Reduces the overfitting problem because it takes the average of all the predictions, which cancels out the biases.\n",
        "- We can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
        "\n",
        "But also has disadvantages:\n",
        "- Is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
        "- The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
        "\n",
        "###Gradient Boosting\n",
        "\n",
        "Boosting is a general ensemble technique that involves sequentially adding models to the ensemble where subsequent models correct the performance of prior models. Gradient boosting classifiers are machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.\n",
        "\n",
        "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network.\n",
        "\n",
        "Gradient boosting is often the main, or one of the main, algorithms used to win machine learning competitions like Kaggle on tabular and similar structured datasets because of its efficiency.\n",
        "\n",
        "## Final evaluation and conclusion\n",
        "\n",
        "We will use three different metrics to evaluate the performance of our classifiers. This is because we are dealing with a highly imbalanced dataset, and calculating just precision would be misleading for us. Therefore we included recall and accuracy, which give us a sense on how much of the least found category we are able to detect.\n",
        "\n",
        "After training both of the classifiers we obtain the next metrics:\n",
        "\n",
        "Random Forest:\n",
        "- Precision: 1.0 \n",
        "- Recall: 0.81 \n",
        "- Accuracy: 0.975\n",
        "\n",
        "Gradient Boosting:\n",
        "- Precision: 0.889 \n",
        "- Recall: 0.816 \n",
        "- Accuracy: 0.962\n",
        "\n",
        "We can observe that Random Forest prevailed at two of the three metrics, with impeccable precision. Therefore, if we can deal with all the possible disadvantages, is the algorithm that we would consider to be the best. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zinfbRGU2sq5",
        "colab_type": "text"
      },
      "source": [
        "### Read in & clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuAgE1KqogQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries \n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbFACN9h_Wc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d41ddbbc-8c7c-4834-c669-83cd4101cddb"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWwxQvTbojN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading stopwords \n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR16xqwS_Y5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemmer\n",
        "ps = nltk.PorterStemmer()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHbG5O-s_dyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading SMS text data\n",
        "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
        "data.columns = ['label', 'body_text']\n",
        "\n",
        "# calculates ratio of punctuation to letters \n",
        "def count_punct(text):\n",
        "    count = sum([1 for char in text if char in string.punctuation]) #count each punctation and sums them up\n",
        "    return round(count/(len(text) - text.count(\" \")), 3)*100 # gives percentage of text that is punctuation\n",
        "\n",
        "data['text_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \")) # count the number of letters not space\n",
        "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x)) #give percentage of punctation"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQZQG3g8AQZ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ec229e97-7637-4e28-a58d-dc10f5ef793d"
      },
      "source": [
        "# Value count of spam vs ham\n",
        "data['label'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4821\n",
              "spam     746\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5C2zFXrDHZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a7f4d57-e867-48a0-82db-158cda6d7170"
      },
      "source": [
        "# Check percentages\n",
        "print(\"Percentage of ham: {}\".format(round(data['label'].value_counts()[0]/len(data['label']),3)))\n",
        "print(\"Percentage of spam: {}\".format(round(data['label'].value_counts()[1]/len(data['label']),3)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of ham: 0.866\n",
            "Percentage of spam: 0.134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6-LsW1A_hAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2598e79d-e589-44f1-c2d4-22bdb38bdd81"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>text_len</th>\n",
              "      <th>punct%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>128</td>\n",
              "      <td>4.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>49</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "      <td>62</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>28</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "      <td>135</td>\n",
              "      <td>4.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                          body_text  text_len  punct%\n",
              "0  spam  Free entry in 2 a wkly comp to win FA Cup fina...       128     4.7\n",
              "1   ham  Nah I don't think he goes to usf, he lives aro...        49     4.1\n",
              "2   ham  Even my brother is not like to speak with me. ...        62     3.2\n",
              "3   ham                I HAVE A DATE ON SUNDAY WITH WILL!!        28     7.1\n",
              "4   ham  As per your request 'Melle Melle (Oru Minnamin...       135     4.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OCVv6xMD07U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "3558315f-3296-40e5-eac7-c63a95b9009f"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_len</th>\n",
              "      <th>punct%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5567.000000</td>\n",
              "      <td>5567.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>65.762170</td>\n",
              "      <td>7.098545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>48.808397</td>\n",
              "      <td>6.631088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>3.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>99.000000</td>\n",
              "      <td>9.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>740.000000</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          text_len       punct%\n",
              "count  5567.000000  5567.000000\n",
              "mean     65.762170     7.098545\n",
              "std      48.808397     6.631088\n",
              "min       2.000000     0.000000\n",
              "25%      29.000000     3.300000\n",
              "50%      50.000000     5.500000\n",
              "75%      99.000000     9.100000\n",
              "max     740.000000   100.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5i3WdIa2sq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuation\n",
        "def clean_text(text):\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])#join words not in punctuation\n",
        "    tokens = re.split('\\W+', text)# tokenaize words separated by white space \n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords] # find the stem of each word and return a list\n",
        "    return text"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0cSlWeA2srF",
        "colab_type": "text"
      },
      "source": [
        "### Split into train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKpHNWga2srF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'text_len', 'punct%']], data['label'], test_size=0.2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQ2vjnU2srK",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWEPTEJL2srP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "93b02c98-583d-43b5-87dc-c999eadb00e1"
      },
      "source": [
        "# use Tfid to convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "#intializing the victorizer modul\n",
        "tfidf_vect = TfidfVectorizer(analyzer=clean_text)# Analyzer takes a function that handles preprocessing, tokenization and n-grams generation.\n",
        "\n",
        "# Learn vocabulary  from training set\n",
        "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text']) \n",
        "\n",
        "#takes learned vocabulary and transforms it to Tf-idf-weighted document-term matrix\n",
        "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
        "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
        "\n",
        "#change from document-term matrix to pd dataframe showing text length and punct%\n",
        "X_train_vect = pd.concat([X_train[['text_len', 'punct%']].reset_index(drop=True), \n",
        "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
        "X_test_vect = pd.concat([X_test[['text_len', 'punct%']].reset_index(drop=True), \n",
        "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
        "\n",
        "X_train_vect.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_len</th>\n",
              "      <th>punct%</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>7223</th>\n",
              "      <th>7224</th>\n",
              "      <th>7225</th>\n",
              "      <th>7226</th>\n",
              "      <th>7227</th>\n",
              "      <th>7228</th>\n",
              "      <th>7229</th>\n",
              "      <th>7230</th>\n",
              "      <th>7231</th>\n",
              "      <th>7232</th>\n",
              "      <th>7233</th>\n",
              "      <th>7234</th>\n",
              "      <th>7235</th>\n",
              "      <th>7236</th>\n",
              "      <th>7237</th>\n",
              "      <th>7238</th>\n",
              "      <th>7239</th>\n",
              "      <th>7240</th>\n",
              "      <th>7241</th>\n",
              "      <th>7242</th>\n",
              "      <th>7243</th>\n",
              "      <th>7244</th>\n",
              "      <th>7245</th>\n",
              "      <th>7246</th>\n",
              "      <th>7247</th>\n",
              "      <th>7248</th>\n",
              "      <th>7249</th>\n",
              "      <th>7250</th>\n",
              "      <th>7251</th>\n",
              "      <th>7252</th>\n",
              "      <th>7253</th>\n",
              "      <th>7254</th>\n",
              "      <th>7255</th>\n",
              "      <th>7256</th>\n",
              "      <th>7257</th>\n",
              "      <th>7258</th>\n",
              "      <th>7259</th>\n",
              "      <th>7260</th>\n",
              "      <th>7261</th>\n",
              "      <th>7262</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>119</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>136</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>18.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7265 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   text_len  punct%    0    1    2    3  ...  7257  7258  7259  7260  7261  7262\n",
              "0       119     1.7  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "1        66     6.1  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "2       136     4.4  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "3        20     5.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "4        44    18.2  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 7265 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhqVwker2srV",
        "colab_type": "text"
      },
      "source": [
        "### Final evaluation of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvXbBRXg2srb",
        "colab_type": "code",
        "colab": {},
        "outputId": "3ed81276-6201-4490-acce-409d4248a9e4"
      },
      "source": [
        "#defining model \n",
        "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
        "\n",
        "#training model and recording how long it takes to train\n",
        "start = time.time()\n",
        "rf_model = rf.fit(X_train_vect, y_train)\n",
        "end = time.time()\n",
        "fit_time = (end - start)\n",
        "\n",
        "# predicting for X_test_vect and recording how long it took to predict \n",
        "start = time.time()\n",
        "y_pred = rf_model.predict(X_test_vect)\n",
        "end = time.time()\n",
        "pred_time = (end - start)\n",
        "\n",
        "#displaying stas\n",
        "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 1.782 / Predict time: 0.213 ---- Precision: 1.0 / Recall: 0.81 / Accuracy: 0.975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBi7MCTb2srf",
        "colab_type": "code",
        "colab": {},
        "outputId": "a8980cd0-7d5a-4029-c468-ed93787a774e"
      },
      "source": [
        "#defining model \n",
        "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
        "#training model and recording how long it takes to train\n",
        "start = time.time()\n",
        "gb_model = gb.fit(X_train_vect, y_train)\n",
        "end = time.time()\n",
        "fit_time = (end - start)\n",
        "# predicting for X_test_vect and recording how long it took to predict \n",
        "start = time.time()\n",
        "y_pred = gb_model.predict(X_test_vect)\n",
        "end = time.time()\n",
        "pred_time = (end - start)\n",
        "#displaying stas\n",
        "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 186.61 / Predict time: 0.135 ---- Precision: 0.889 / Recall: 0.816 / Accuracy: 0.962\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}